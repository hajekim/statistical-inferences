{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This will fail since we are not importing os and calling os.walk\n",
    "def fetch_documents(top_directory):\n",
    "    \"\"\"\n",
    "    Generator: iterate over all relevant documents, yielding one\n",
    "    document (=list of utf8 tokens) at a time.\n",
    "    \"\"\"\n",
    "    # find all .txt documents, no matter how deep under top_directory\n",
    "    for root, dirs, files in os.walk(top_directory):\n",
    "        for fname in filter(lambda fname: fname.endswith('.txt'), files):\n",
    "            # read each document as one big string\n",
    "            return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-057711b101bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexec_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./dataset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(obj)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-70ba1ef70855>\u001b[0m in \u001b[0;36mfetch_documents\u001b[1;34m(top_directory)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# find all .txt documents, no matter how deep under top_directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;31m# read each document as one big string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "exec_obj = fetch_documents(\"./dataset\")\n",
    "#print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This will not fail since the evaluation is lazy \n",
    "def iter_documents(top_directory):\n",
    "    \"\"\"\n",
    "    Generator: iterate over all relevant documents, yielding one\n",
    "    document (=list of utf8 tokens) at a time.\n",
    "    \"\"\"\n",
    "    # find all .txt documents, no matter how deep under top_directory\n",
    "    for root, dirs, files in os.walk(top_directory):\n",
    "        for fname in filter(lambda fname: fname.endswith('.txt'), files):\n",
    "            # read each document as one big string\n",
    "            yield fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object iter_documents at 0x0000028777A0AE60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_obj = iter_documents(\"./dataset\")\n",
    "iter_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-15424872c945>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## this will now fail when we try to fetch doc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-55cfd96c5db8>\u001b[0m in \u001b[0;36miter_documents\u001b[1;34m(top_directory)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# find all .txt documents, no matter how deep under top_directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;31m# read each document as one big string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "## this will now fail when we try to fetch doc\n",
    "next(iter_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets do something meaningful... \n",
    "\n",
    "The task is tokenize all the .txt files in the dataset directory... \n",
    "\n",
    "#### Why tokenization ???\n",
    "All the Algorthims from Bag of Words, to LDA to Word2Vec take tokens as the input... \n",
    "\n",
    "We need to transform our text files so that we can pass them into Algos in downstream operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_documents(top_directory):\n",
    "    \"\"\"\n",
    "    Generator: iterate over all relevant documents, yielding one\n",
    "    document (=list of utf8 tokens) at a time.\n",
    "    \"\"\"\n",
    "    # find all .txt documents, no matter how deep under top_directory\n",
    "    for root, dirs, files in os.walk(top_directory):\n",
    "        for fname in filter(lambda fname: fname.endswith('.txt'), files):\n",
    "            # read each document as one big string\n",
    "            document = open(os.path.join(root, fname)).read()\n",
    "            # break document into utf8 tokens\n",
    "            yield gensim.utils.tokenize(document, lower=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We need to create a class that overrides __iter__ else we Generators is the only solution  \n",
    "\n",
    "What are Generators ??? Language provided Iterators that can be used only once.. refer to the notebook on Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TxtSubdirsCorpus(object):\n",
    "    \"\"\"\n",
    "    Iterable: on each iteration, return bag-of-words vectors,\n",
    "    one vector for each document.\n",
    " \n",
    "    Process one document at a time using generators, never\n",
    "    load the entire corpus into RAM.\n",
    " \n",
    "    \"\"\"\n",
    "    def __init__(self, top_dir):\n",
    "        self.top_dir = top_dir\n",
    "        # create dictionary = mapping for documents => sparse vectors\n",
    "        self.dictionary = gensim.corpora.Dictionary(iter_documents(top_dir))\n",
    " \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Again, __iter__ is a generator => TxtSubdirsCorpus is a streamed iterable.\n",
    "        \"\"\"\n",
    "        for tokens in iter_documents(self.top_dir):\n",
    "            # transform tokens (strings) into a sparse vector, one at a time\n",
    "            yield self.dictionary.doc2bow(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# that's it! the streamed corpus of sparse vectors is ready\n",
    "corpus = TxtSubdirsCorpus('./dataset/')\n",
    " \n",
    "# print the corpus vectors\n",
    "for vector in corpus:\n",
    "    print vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
